# ðŸ§  Transformer from Scratch ðŸš€

This project demonstrates a complete implementation of the Transformer architecture from scratch using PyTorch â€” no Hugging Face, no shortcuts. It's built for learning and understanding the internals of how models like GPT and BERT work at the foundational level.

## ðŸ“Œ Whatâ€™s Included?

- âœ… Multi-Head Self Attention  
- âœ… Transformer Encoder & Decoder Blocks  
- âœ… Positional Embeddings  
- âœ… Causal and Padding Masking  
- âœ… Full Transformer Class (Encoder-Decoder)  
- âœ… Dummy Training Data and Forward Pass  
- âœ… Fully Commented and Colab-Friendly Code  
- âœ… Compatible with CPU or CUDA

---

## ðŸ“‚ Project Structure

â”œâ”€â”€ transformer_from_scratch.ipynb 
          # Complete implementation
â””â”€â”€ README.md 
          # project documentation

---

## ðŸš€ Quick Start

### Clone and Run

```bash
git clone https://github.com/iamnarendran/Transformer-from-scratch.git
cd Transformer-from-scratch
python transformer_from_scratch.py
```

---

## Output Example

```
torch.Size([2, 7, 10])
```

| 2 â†’ batch size

| 7 â†’ output sequence length (target without last token)

| 10 â†’ logits for each token in vocab (vocab size = 10)